#!/usr/bin/env python

#
# genomicsdb_query python script
#
# The MIT License
#
# Copyright (c) 2024 dātma, inc™
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#

import argparse
import json
import os
import re
import sys

import genomicsdb_common
import pyarrow as pa
import pyarrow.parquet as pq

import genomicsdb
from genomicsdb import json_output_mode
from genomicsdb.protobuf import genomicsdb_coordinates_pb2 as query_coords
from genomicsdb.protobuf import genomicsdb_export_config_pb2 as query_pb


def parse_callset_json(callset_file):
    callset = json.loads(genomicsdb.read_entire_file(callset_file))
    callsets = callset["callsets"]
    samples = [callset if isinstance(callset, str) else callset["sample_name"] for callset in callsets]
    return samples


def parse_callset_json_for_row_ranges(callset_file, samples=None):
    if not samples:
        return None
    callset = json.loads(genomicsdb.read_entire_file(callset_file))
    callsets = callset["callsets"]
    if isinstance(samples, str):
        with open(samples) as file:
            samples = [line.rstrip() for line in file]
    if not samples:
        return None
    # Old style json has type(callset) == str whereas the one generated with vcf2genomicsdb_init is a list
    rows = [
        callsets[callset]["row_idx"] if isinstance(callset, str) else callset["row_idx"]
        for sample in samples
        for callset in callsets
        if (not isinstance(callset, str) and sample == callset["sample_name"])
        or (isinstance(callset, str) and sample == callset)
    ]
    if len(rows) == 0:
        print(f"None of the samples{samples} specified were found in the workspace")
        return []
    rows.sort(reverse=True)
    # make row_tuples
    row = rows.pop()
    row_tuples = [(row, row)]
    while len(rows) > 0:
        row = rows.pop()
        tuple = row_tuples[-1]
        if row > tuple[1] + 1:
            # Append new range
            row_tuples.append((row, row))
        elif row == tuple[1] + 1:
            # Expand range for tuple
            row_tuples.pop()
            row_tuples.append((tuple[0], row))
    return row_tuples


def parse_loader_json(loader_file):
    loader = json.loads(genomicsdb.read_entire_file(loader_file))
    partitions = loader["column_partitions"]
    array_names = [
        partition["array_name"] if "array_name" in partition else partition["array"] for partition in partitions
    ]
    return [name.replace("$", ":", 1).replace("$", "-", 1) for name in array_names]


def genomicsdb_connect(workspace, callset_file, vidmap_file, filter):
    export_config = query_pb.ExportConfiguration()
    export_config.workspace = workspace
    export_config.attributes.extend(["REF", "ALT", "GT"])
    export_config.vid_mapping_file = vidmap_file
    export_config.callset_mapping_file = callset_file
    export_config.bypass_intersecting_intervals_phase = False
    export_config.enable_shared_posixfs_optimizations = True
    if filter:
        export_config.query_filter = filter
    return genomicsdb.connect_with_protobuf(export_config)


def setup_gdb():
    parser = argparse.ArgumentParser(
        prog="query",
        description="GenomicsDB simple query with samples/intervals/filter as inputs",
        formatter_class=argparse.RawTextHelpFormatter,
        usage="%(prog)s [options]",
    )
    parser.add_argument(
        "--version",
        action="version",
        version=genomicsdb.version(),
        help="print GenomicsDB native library version and exit",
    )
    parser.add_argument(
        "-w",
        "--workspace",
        required=True,
        help="URL to GenomicsDB workspace \ne.g. -w my_workspace or -w az://my_container/my_workspace"
        + " or -w s3://my_bucket/my_workspace or -w gs://my_bucket/my_workspace",
    )
    parser.add_argument(
        "-v",
        "--vidmap",
        required=False,
        help="Optional - URL to vid mapping file. Defaults to vidmap.json in workspace",
    )
    parser.add_argument(
        "-c",
        "--callset",
        required=False,
        help="Optional - URL to callset mapping file. Defaults to callset.json in workspace",
    )
    parser.add_argument(
        "-l", "--loader", required=False, help="Optional - URL to loader file. Defaults to loader.json in workspace"
    )
    parser.add_argument("--list-samples", action="store_true", help="List samples ingested into the workspace and exit")
    parser.add_argument(
        "--list-contigs", action="store_true", help="List contigs for the ingested samples in the workspace and exit"
    )
    parser.add_argument(
        "--list-partitions",
        action="store_true",
        help="List interval partitions(genomicsdb arrays in the workspace) for the ingested samples in the workspace and exit",  # noqa
    )
    parser.add_argument(
        "-i",
        "--interval",
        action="append",
        required=False,
        help="genomic intervals over which to operate. The intervals should be specified in the <CONTIG>:<START>-<END> format with START and END optional.\nThis argument may be specified 0 or more times e.g -i chr1:1-10000 -i chr2 -i chr3:1000. \nNote: \n\t1. -i/--interval and -I/--interval-list are mutually exclusive \n\t2. either samples and/or intervals using -i/-I/-s/-S options has to be specified",  # noqa
    )
    parser.add_argument(
        "-I",
        "--interval-list",
        required=False,
        help="genomic intervals listed in a file over which to operate.\nThe intervals should be specified in the <CONTIG>:<START>-<END> format, with START and END optional one interval per line. \nNote: \n\t1. -i/--interval and -I/--interval-list are mutually exclusive \n\t2. either samples and/or intervals using -i/-I/-s/-S options has to be specified",  # noqa
    )
    parser.add_argument(
        "-s",
        "--sample",
        action="append",
        required=False,
        help="sample names over which to operate. This argument may be specified 0 or more times e.g -s HG00097 -s HG00090. \nNote: \n\t1. -s/--sample and -S/--sample-list are mutually exclusive \n\t2. either samples and/or intervals using -i/-I/-s/-S options has to be specified",  # noqa
    )
    parser.add_argument(
        "-S",
        "--sample-list",
        required=False,
        help="sample file containing list of samples, one sample per line, to operate upon. \nNote: \n\t1. -s/--sample and -S/--sample-list are mutually exclusive \n\t2. either samples and/or intervals using -i/-I/-s/-S options has to be specified",  # noqa
    )
    parser.add_argument(
        "-f",
        "--filter",
        required=False,
        help="Optional - genomic filter expression for the query, e.g. 'ISHOMREF' or 'ISHET' or 'REF == \"G\" && resolve(GT, REF, ALT) &= \"T/T\" && ALT |= \"T\"'",  # noqa
    )
    parser.add_argument(
        "-t",
        "--output-type",
        choices=["csv", "json", "arrow"],
        default="csv",
        help="Optional - specify type of output for the query (default: %(default)s)",
    )
    parser.add_argument(
        "-j",
        "--json-output-type",
        choices=["all", "all-by-calls", "samples-with-num-calls", "samples", "num-calls"],
        default="samples-with-num-calls",
        help="Optional - used in conjunction with -t/--output-type json (default: %(default)s)",
    )
    parser.add_argument(
        "-z",
        "--max-arrow-byte-size",
        default="64MB",
        help="Optional - used in conjunction with -t/--output-type arrow as hint for buffering parquet files(default: %(default)s)",  # noqa
    )
    parser.add_argument(
        "-o",
        "--output",
        default="query_output",
        help="a prefix filename to csv outputs from the tool. The filenames will be suffixed with the interval and .csv/.json (default: %(default)s)",  # noqa
    )

    args = parser.parse_args()

    workspace = genomicsdb_common.normalize_path(args.workspace)
    if not genomicsdb.workspace_exists(workspace):
        raise RuntimeError(f"workspace({workspace}) not found")
    callset_file = args.callset
    if not callset_file:
        callset_file = workspace + "/callset.json"
    vidmap_file = args.vidmap
    if not vidmap_file:
        vidmap_file = workspace + "/vidmap.json"
    loader_file = args.loader
    if not loader_file:
        loader_file = workspace + "/loader.json"
    if (
        not genomicsdb.is_file(callset_file)
        or not genomicsdb.is_file(vidmap_file)
        or not genomicsdb.is_file(loader_file)
    ):
        raise RuntimeError(f"callset({callset_file}) vidmap({vidmap_file}) or loader({loader_file}) not found")

    if args.list_contigs:
        _, intervals = genomicsdb_common.parse_vidmap_json(vidmap_file)
        print(*intervals, sep="\n")
        sys.exit(0)

    if args.list_samples:
        samples = parse_callset_json(callset_file)
        print(*samples, sep="\n")
        sys.exit(0)

    if args.list_partitions:
        contigs_map, _ = genomicsdb_common.parse_vidmap_json(vidmap_file)
        # parse loader.json for partitions
        partitions = parse_loader_json(loader_file)
        print(*partitions, sep="\n")
        sys.exit(0)

    intervals = args.interval
    interval_list = args.interval_list
    samples = args.sample
    sample_list = args.sample_list
    if not intervals and not samples and not interval_list and not sample_list:
        raise RuntimeError(
            "Specify at least one of either -i/-interval -I/--interval-list -s/--sample -S/--sample-list has to be specified"  # noqa
        )

    contigs_map, intervals = genomicsdb_common.parse_vidmap_json(vidmap_file, intervals or interval_list)
    row_tuples = parse_callset_json_for_row_ranges(callset_file, samples or sample_list)

    # parse loader.json for partitions
    loader = json.loads(genomicsdb.read_entire_file(loader_file))
    partitions = loader["column_partitions"]

    gdb = genomicsdb_connect(workspace, callset_file, vidmap_file, args.filter)

    return gdb, workspace, partitions, contigs_map, intervals, row_tuples, args


def generate_output_filename(output, output_type, interval, idx):
    if output_type == "arrow":
        output_filename = os.path.join(output, f"{interval.replace(':', '-')}")
    else:
        output_filename = f"{output}_{interval.replace(':', '-')}"
    if idx > 0:
        output_filename = output_filename + f"_{idx}"
    if output_type == "arrow":
        return output_filename
    else:
        return output_filename + "." + output_type


def parse_args_for_json_type(json_output_type):
    json_types = {
        "all": json_output_mode.ALL,
        "all-by-calls": json_output_mode.ALL_BY_CALLS,
        "samples-with-num-calls": json_output_mode.SAMPLES_WITH_NUM_CALLS,
        "samples": json_output_mode.SAMPLES,
        "num-calls": json_output_mode.NUM_CALLS,
    }
    return json_types[json_output_type]


def parse_args_for_max_bytes(max_arrow_byte_size):
    max_arrow_byte_size = max_arrow_byte_size.upper()
    units = {"B": 1, "KB": 1024, "MB": 1024**2, "GB": 1024**3, "TB": 1024**4}
    if not re.match(r" ", max_arrow_byte_size):
        max_arrow_byte_size = re.sub(r"([KMGT]?B)", r" \1", max_arrow_byte_size)
    number, unit = max_arrow_byte_size.split()
    return int(float(number) * units[unit])


def main():
    gdb, workspace, partitions, contigs_map, intervals, row_tuples, args = setup_gdb()

    if row_tuples and len(row_tuples) == 0:
        return

    print(f"Starting genomicsdb_query for workspace({workspace}) and intervals({intervals})")

    row_range_list = None
    if row_tuples:
        row_range_list = query_pb.RowRangeList()
        for tuple in row_tuples:
            range = query_pb.RowRange()
            range.low = tuple[0]
            range.high = tuple[1]
            row_range_list.range_list.extend([range])

    output_type = args.output_type
    output = args.output
    if output_type == "json":
        json_type = parse_args_for_json_type(args.json_output_type)
    if output_type == "arrow":
        max_arrow_bytes = parse_args_for_max_bytes(args.max_arrow_byte_size)
        print(f"Using {args.max_arrow_byte_size} number of bytes as hint for writing out parquet files")

    for interval in intervals:
        print(f"Processing interval({interval})...")

        contig, start, end, arrays = genomicsdb_common.get_arrays(interval, contigs_map, partitions)
        if len(arrays) == 0:
            print(f"No arrays in the workspace matched input interval({interval})")
            continue

        print(f"\tArrays:{arrays} under consideration for interval({interval})")

        for idx, array in enumerate(arrays):
            if not genomicsdb.array_exists(workspace, array):
                print(f"\tArray({array}) not imported into workspace({workspace}) for interval({interval})")
                continue
            query_config = query_pb.QueryConfiguration()
            query_config.array_name = array
            contig_interval = query_coords.ContigInterval()
            contig_interval.contig = contig
            contig_interval.begin = start
            contig_interval.end = end
            query_config.query_contig_intervals.extend([contig_interval])
            if row_range_list:
                query_config.query_row_ranges.extend([row_range_list])
            output_filename = generate_output_filename(output, output_type, interval, idx)
            if output_type == "csv":
                df = gdb.query_variant_calls(query_protobuf=query_config, flatten_intervals=True)
                df.to_csv(output_filename, index=False)
            elif output_type == "json":
                json_output = gdb.query_variant_calls(query_protobuf=query_config, json_output=json_type)
                with open(output_filename, "wb") as f:
                    f.write(json_output)
            elif output_type == "arrow":
                if not os.path.exists(output):
                    os.mkdir(output)
                nbytes = 0
                writer = None
                i = 0
                for out in gdb.query_variant_calls(query_protobuf=query_config, arrow_output=True, batching=True):
                    reader = pa.ipc.open_stream(out)
                    for batch in reader:
                        if nbytes > max_arrow_bytes:
                            i += 1
                            nbytes = 0
                            if writer:
                                writer.close()
                                writer = None
                        if not writer:
                            print(f"Writing out batch {i}...")
                            writer = pq.ParquetWriter(f"{output_filename}__{i}.parquet", batch.schema)
                        nbytes += batch.nbytes
                        writer.write_batch(batch)
                if writer:
                    writer.close()

    print(f"genomicsdb_query for workspace({workspace}) and intervals({intervals}) completed successfully")


if __name__ == "__main__":
    main()
