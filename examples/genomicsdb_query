#!/usr/bin/env python

#
# genomicsdb_query python script
#
# The MIT License
#
# Copyright (c) 2024 dātma, inc™
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#

import argparse
import json
import logging
import multiprocessing
import os
import re
import sys
from typing import List, NamedTuple

import genomicsdb_common
import pyarrow as pa
import pyarrow.parquet as pq

import genomicsdb
from genomicsdb import json_output_mode
from genomicsdb.protobuf import genomicsdb_coordinates_pb2 as query_coords
from genomicsdb.protobuf import genomicsdb_export_config_pb2 as query_pb

logging.basicConfig(
    format="%(asctime)s.%(msecs)03d %(levelname)-5s GenomicsDB Python - pid=%(process)d tid=%(thread)d %(message)s",
    level=logging.INFO,
    datefmt="%H:%M:%S",
)


def parse_callset_json(callset_file):
    callset = json.loads(genomicsdb.read_entire_file(callset_file))
    callsets = callset["callsets"]
    samples = [callset if isinstance(callset, str) else callset["sample_name"] for callset in callsets]
    return samples


def parse_callset_json_for_row_ranges(callset_file, samples=None):
    if not samples:
        return None
    callset = json.loads(genomicsdb.read_entire_file(callset_file))
    callsets = callset["callsets"]
    if isinstance(samples, str):
        with open(samples) as file:
            samples = [line.rstrip() for line in file]
    if not samples:
        return None
    # Old style json has type(callset) == str whereas the one generated with vcf2genomicsdb_init is a list
    rows = [
        callsets[callset]["row_idx"] if isinstance(callset, str) else callset["row_idx"]
        for sample in samples
        for callset in callsets
        if (not isinstance(callset, str) and sample == callset["sample_name"])
        or (isinstance(callset, str) and sample == callset)
    ]
    if len(rows) == 0:
        print(f"None of the samples{samples} specified were found in the workspace")
        return []
    rows.sort(reverse=True)
    # make row_tuples
    row = rows.pop()
    row_tuples = [(row, row)]
    while len(rows) > 0:
        row = rows.pop()
        tuple = row_tuples[-1]
        if row > tuple[1] + 1:
            # Append new range
            row_tuples.append((row, row))
        elif row == tuple[1] + 1:
            # Expand range for tuple
            row_tuples.pop()
            row_tuples.append((tuple[0], row))
    return row_tuples


def parse_loader_json(loader_file):
    loader = json.loads(genomicsdb.read_entire_file(loader_file))
    partitions = loader["column_partitions"]
    array_names = [
        partition["array_name"] if "array_name" in partition else partition["array"] for partition in partitions
    ]
    return [name.replace("$", ":", 1).replace("$", "-", 1) for name in array_names]


def check_nproc(value):
    ivalue = int(value)
    if ivalue < 1:
        raise argparse.ArgumentTypeError("%s specified nproc arg cannot be less than 1" % value)
    elif ivalue > multiprocessing.cpu_count():
        raise argparse.ArgumentTypeError(
            "%s specified nproc arg cannot exceed max number of available processing units" % value
        )
    return ivalue


def setup():
    parser = argparse.ArgumentParser(
        prog="query",
        description="GenomicsDB simple query with samples/intervals/filter as inputs",
        formatter_class=argparse.RawTextHelpFormatter,
        usage="%(prog)s [options]",
    )
    parser.add_argument(
        "--version",
        action="version",
        version=genomicsdb.version(),
        help="print GenomicsDB native library version and exit",
    )
    parser.add_argument(
        "-w",
        "--workspace",
        required=True,
        help="URL to GenomicsDB workspace \ne.g. -w my_workspace or -w az://my_container/my_workspace"
        + " or -w s3://my_bucket/my_workspace or -w gs://my_bucket/my_workspace",
    )
    parser.add_argument(
        "-v",
        "--vidmap",
        required=False,
        help="Optional - URL to vid mapping file. Defaults to vidmap.json in workspace",
    )
    parser.add_argument(
        "-c",
        "--callset",
        required=False,
        help="Optional - URL to callset mapping file. Defaults to callset.json in workspace",
    )
    parser.add_argument(
        "-l", "--loader", required=False, help="Optional - URL to loader file. Defaults to loader.json in workspace"
    )
    parser.add_argument("--list-samples", action="store_true", help="List samples ingested into the workspace and exit")
    parser.add_argument(
        "--list-contigs", action="store_true", help="List contigs for the ingested samples in the workspace and exit"
    )
    parser.add_argument(
        "--list-partitions",
        action="store_true",
        help="List interval partitions(genomicsdb arrays in the workspace) for the ingested samples in the workspace and exit",  # noqa
    )
    parser.add_argument(
        "-i",
        "--interval",
        action="append",
        required=False,
        help="genomic intervals over which to operate. The intervals should be specified in the <CONTIG>:<START>-<END> format with START and END optional.\nThis argument may be specified 0 or more times e.g -i chr1:1-10000 -i chr2 -i chr3:1000. \nNote: \n\t1. -i/--interval and -I/--interval-list are mutually exclusive \n\t2. either samples and/or intervals using -i/-I/-s/-S options has to be specified",  # noqa
    )
    parser.add_argument(
        "-I",
        "--interval-list",
        required=False,
        help="genomic intervals listed in a file over which to operate.\nThe intervals should be specified in the <CONTIG>:<START>-<END> format, with START and END optional one interval per line. \nNote: \n\t1. -i/--interval and -I/--interval-list are mutually exclusive \n\t2. either samples and/or intervals using -i/-I/-s/-S options has to be specified",  # noqa
    )
    parser.add_argument(
        "-s",
        "--sample",
        action="append",
        required=False,
        help="sample names over which to operate. This argument may be specified 0 or more times e.g -s HG00097 -s HG00090. \nNote: \n\t1. -s/--sample and -S/--sample-list are mutually exclusive \n\t2. either samples and/or intervals using -i/-I/-s/-S options has to be specified",  # noqa
    )
    parser.add_argument(
        "-S",
        "--sample-list",
        required=False,
        help="sample file containing list of samples, one sample per line, to operate upon. \nNote: \n\t1. -s/--sample and -S/--sample-list are mutually exclusive \n\t2. either samples and/or intervals using -i/-I/-s/-S options has to be specified",  # noqa
    )
    parser.add_argument(
        "-f",
        "--filter",
        required=False,
        help="Optional - genomic filter expression for the query, e.g. 'ISHOMREF' or 'ISHET' or 'REF == \"G\" && resolve(GT, REF, ALT) &= \"T/T\" && ALT |= \"T\"'",  # noqa
    )
    parser.add_argument(
        "-n",
        "--nproc",
        type=check_nproc,
        default=8,
        help="Optional - number of processing units for multiprocessing(default: %(default)s). Run nproc from command line to print the number of processing units available to a process for the user",  # noqa
    )
    parser.add_argument(
        "-t",
        "--output-type",
        choices=["csv", "json", "arrow"],
        default="csv",
        help="Optional - specify type of output for the query (default: %(default)s)",
    )
    parser.add_argument(
        "-j",
        "--json-output-type",
        choices=["all", "all-by-calls", "samples-with-num-calls", "samples", "num-calls"],
        default="samples-with-num-calls",
        help="Optional - used in conjunction with -t/--output-type json (default: %(default)s)",
    )
    parser.add_argument(
        "-z",
        "--max-arrow-byte-size",
        default="64MB",
        help="Optional - used in conjunction with -t/--output-type arrow as hint for buffering parquet files(default: %(default)s)",  # noqa
    )
    parser.add_argument(
        "-o",
        "--output",
        default="query_output",
        help="a prefix filename to csv outputs from the tool. The filenames will be suffixed with the interval and .csv/.json (default: %(default)s)",  # noqa
    )

    args = parser.parse_args()

    workspace = genomicsdb_common.normalize_path(args.workspace)
    if not genomicsdb.workspace_exists(workspace):
        raise RuntimeError(f"workspace({workspace}) not found")
    callset_file = args.callset
    if not callset_file:
        callset_file = workspace + "/callset.json"
    vidmap_file = args.vidmap
    if not vidmap_file:
        vidmap_file = workspace + "/vidmap.json"
    loader_file = args.loader
    if not loader_file:
        loader_file = workspace + "/loader.json"
    if (
        not genomicsdb.is_file(callset_file)
        or not genomicsdb.is_file(vidmap_file)
        or not genomicsdb.is_file(loader_file)
    ):
        raise RuntimeError(f"callset({callset_file}) vidmap({vidmap_file}) or loader({loader_file}) not found")

    if args.list_contigs:
        _, intervals = genomicsdb_common.parse_vidmap_json(vidmap_file)
        print(*intervals, sep="\n")
        sys.exit(0)

    if args.list_samples:
        samples = parse_callset_json(callset_file)
        print(*samples, sep="\n")
        sys.exit(0)

    if args.list_partitions:
        contigs_map, _ = genomicsdb_common.parse_vidmap_json(vidmap_file)
        # parse loader.json for partitions
        partitions = parse_loader_json(loader_file)
        print(*partitions, sep="\n")
        sys.exit(0)

    intervals = args.interval
    interval_list = args.interval_list
    samples = args.sample
    sample_list = args.sample_list
    if not intervals and not samples and not interval_list and not sample_list:
        raise RuntimeError(
            "Specify at least one of either -i/-interval -I/--interval-list -s/--sample -S/--sample-list has to be specified"  # noqa
        )

    contigs_map, intervals = genomicsdb_common.parse_vidmap_json(vidmap_file, intervals or interval_list)
    row_tuples = parse_callset_json_for_row_ranges(callset_file, samples or sample_list)

    # parse loader.json for partitions
    loader = json.loads(genomicsdb.read_entire_file(loader_file))
    partitions = loader["column_partitions"]

    return workspace, callset_file, vidmap_file, partitions, contigs_map, intervals, row_tuples, args


def generate_output_filename(output, output_type, interval, idx):
    if output_type == "arrow":
        output_filename = os.path.join(output, f"{interval.replace(':', '-')}")
    else:
        output_filename = f"{output}_{interval.replace(':', '-')}"
    if idx > 0:
        output_filename = output_filename + f"_{idx}"
    if output_type == "arrow":
        return output_filename
    else:
        return output_filename + "." + output_type


def parse_args_for_json_type(json_output_type):
    json_types = {
        "all": json_output_mode.ALL,
        "all-by-calls": json_output_mode.ALL_BY_CALLS,
        "samples-with-num-calls": json_output_mode.SAMPLES_WITH_NUM_CALLS,
        "samples": json_output_mode.SAMPLES,
        "num-calls": json_output_mode.NUM_CALLS,
    }
    return json_types[json_output_type]


def parse_args_for_max_bytes(max_arrow_byte_size):
    max_arrow_byte_size = max_arrow_byte_size.upper()
    units = {"B": 1, "KB": 1024, "MB": 1024**2, "GB": 1024**3, "TB": 1024**4}
    if not re.match(r" ", max_arrow_byte_size):
        max_arrow_byte_size = re.sub(r"([KMGT]?B)", r" \1", max_arrow_byte_size)
    number, unit = max_arrow_byte_size.split()
    return int(float(number) * units[unit])


class GenomicsDBExportConfig(NamedTuple):
    workspace: str
    vidmap_file: str
    callset_file: str
    filter: str


class GenomicsDBQueryConfig(NamedTuple):
    interval: str
    contig: str
    start: int
    end: int
    array_name: str
    row_tuples: List[tuple]


class OutputConfig(NamedTuple):
    filename: str
    type: str
    json_type: str
    max_arrow_bytes: int


class Config(NamedTuple):
    export_config: GenomicsDBExportConfig
    query_config: GenomicsDBQueryConfig
    output_config: OutputConfig


def configure_export(config: GenomicsDBExportConfig):
    export_config = query_pb.ExportConfiguration()
    export_config.workspace = config.workspace
    export_config.attributes.extend(["REF", "ALT", "GT"])
    export_config.vid_mapping_file = config.vidmap_file
    export_config.callset_mapping_file = config.callset_file
    export_config.bypass_intersecting_intervals_phase = False
    export_config.enable_shared_posixfs_optimizations = True
    if config.filter:
        export_config.query_filter = config.filter
    return export_config


def configure_query(config: GenomicsDBQueryConfig):
    query_config = query_pb.QueryConfiguration()
    query_config.array_name = config.array_name
    contig_interval = query_coords.ContigInterval()
    contig_interval.contig = config.contig
    contig_interval.begin = config.start
    contig_interval.end = config.end
    query_config.query_contig_intervals.extend([contig_interval])
    row_range_list = None
    if config.row_tuples:
        row_range_list = query_pb.RowRangeList()
        for tuple in config.row_tuples:
            range = query_pb.RowRange()
            range.low = tuple[0]
            range.high = tuple[1]
            row_range_list.range_list.extend([range])
    if row_range_list:
        query_config.query_row_ranges.extend([row_range_list])
    return query_config


def process(config):
    export_config = config.export_config
    query_config = config.query_config
    output_config = config.output_config
    msg = f"array({query_config.array_name}) for interval({query_config.interval})"
    if not genomicsdb.array_exists(export_config.workspace, query_config.array_name):
        logging.error(msg + f" not imported into workspace({export_config.workspace})")
        return -1
    global gdb
    try:
        if gdb:
            logging.info("Found gdb to process " + msg)
        else:
            logging.error("Something wrong, gdb seems to be None")
            return -1
    except NameError:
        logging.info("Instantiating genomicsdb to process " + msg + "...")
        gdb = genomicsdb.connect_with_protobuf(configure_export(export_config))
        logging.info("Instantiating genomicsdb to process " + msg + " DONE")
    query_protobuf = configure_query(query_config)
    try:
        if output_config.type == "csv":
            df = gdb.query_variant_calls(query_protobuf=query_protobuf, flatten_intervals=True)
            df.to_csv(output_config.filename, index=False)
        elif output_config.type == "json":
            json_output = gdb.query_variant_calls(query_protobuf=query_protobuf, json_output=output_config.json_type)
            with open(output_config.filename, "wb") as f:
                f.write(json_output)
        elif output_config.type == "arrow":
            nbytes = 0
            writer = None
            i = 0
            for out in gdb.query_variant_calls(query_protobuf=query_protobuf, arrow_output=True, batching=True):
                reader = pa.ipc.open_stream(out)
                for batch in reader:
                    if nbytes > output_config.max_arrow_bytes:
                        i += 1
                        nbytes = 0
                        if writer:
                            writer.close()
                            writer = None
                    if not writer:
                        writer = pq.ParquetWriter(f"{output_config.filename}__{i}.parquet", batch.schema)
                    nbytes += batch.nbytes
                    writer.write_batch(batch)
                if writer:
                    writer.close()
                    writer = None
    except Exception as e:
        logging.critical(f"Unexpected exception : {e}")
        gdb = None
        return -1

    logging.info(f"Processed array {query_config.array_name} for interval {query_config.interval}")
    return 0


def main():
    workspace, callset_file, vidmap_file, partitions, contigs_map, intervals, row_tuples, args = setup()

    if row_tuples is not None and len(row_tuples) == 0:
        return

    print(f"Starting genomicsdb_query for workspace({workspace}) and intervals({intervals})")

    output_type = args.output_type
    output = args.output
    json_type = None
    if output_type == "json":
        json_type = parse_args_for_json_type(args.json_output_type)
    max_arrow_bytes = -1
    if output_type == "arrow":
        if not os.path.exists(output):
            os.mkdir(output)
        max_arrow_bytes = parse_args_for_max_bytes(args.max_arrow_byte_size)
        print(f"Using {args.max_arrow_byte_size} number of bytes as hint for writing out parquet files")

    export_config = GenomicsDBExportConfig(workspace, vidmap_file, callset_file, args.filter)
    configs = []
    for interval in intervals:
        print(f"Processing interval({interval})...")

        contig, start, end, arrays = genomicsdb_common.get_arrays(interval, contigs_map, partitions)
        if len(arrays) == 0:
            print(f"No arrays in the workspace matched input interval({interval})")
            continue

        print(f"\tArrays:{arrays} under consideration for interval({interval})")
        for idx, array in enumerate(arrays):
            query_config = GenomicsDBQueryConfig(interval, contig, start, end, array, row_tuples)
            output_config = OutputConfig(
                generate_output_filename(output, output_type, interval, idx),
                output_type,
                json_type,
                max_arrow_bytes,
            )
            configs.append(Config(export_config, query_config, output_config))

    if min(len(configs), args.nproc) == 1:
        results = list(map(process, configs))
    else:
        with multiprocessing.Pool(processes=min(len(configs), args.nproc)) as pool:
            results = list(pool.map(process, configs))

    msg = "successfully"
    for result in results:
        if result != 0:
            msg = "unsuccessfully. Check output for errors"
            break

    print(f"genomicsdb_query for workspace({workspace}) and intervals({intervals}) completed {msg}" )


if __name__ == "__main__":
    main()
